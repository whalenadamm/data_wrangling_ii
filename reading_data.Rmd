---
title: "Data Wrangling II"
subtitle: "Reading Data"
author: "Adam Whalen"
date: "10/28/2020"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(rvest)
library(httr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Scrape a table

I want the first table from [this page](http://samhda.s3-us-gov-west-1.amazonaws.com/s3fs-public/field-uploads/2k15StateFiles/NSDUHsaeShortTermCHG2015.htm).


Read in the html:
```{r}
url = "http://samhda.s3-us-gov-west-1.amazonaws.com/s3fs-public/field-uploads/2k15StateFiles/NSDUHsaeShortTermCHG2015.htm"

drug_use_html = read_html(url)
```

Now, extract the table(s). 

```{r}
tabl_marj = 
  drug_use_html %>% 
  html_nodes(css = "table") %>% 
  first() %>% 
  html_table() %>% 
  slice(-1) %>% 
  as_tibble()
```

## Star Wars Movie info

Let's grab more data: Star Wars movies (title, runtime, gross $). It lives [here](https://www.imdb.com/list/ls070150896/)

```{r}
url = "https://www.imdb.com/list/ls070150896/"

swm_html = read_html(url)
```

Grab the elements that I want.

```{r}
title_vec = 
  swm_html %>% 
  html_nodes(css = ".lister-item-header a") %>% 
  html_text()

gross_rev_vec = 
  swm_html %>% 
  html_nodes(css = ".text-muted .ghost~ .text-muted+ span") %>% 
  html_text()

runtime_vec = 
  swm_html %>% 
  html_nodes(css = ".runtime") %>% 
  html_text()

swm_df = 
  tibble(
    title = title_vec,
    gross_rev = gross_rev_vec,
    runtime = runtime_vec
  )
```

## APIs

### Get some water data

```{r}
nyc_water = 
  GET("https://data.cityofnewyork.us/resource/ia2d-e54m.csv") %>% 
  content("parsed")

nyc_water_json = 
  GET("https://data.cityofnewyork.us/resource/ia2d-e54m.json") %>% 
  content("text") %>% 
  jsonlite::fromJSON() %>% 
  as_tibble()
```

JSON format works fine, but it's not quite as clean or easy. CSV is better.

#### BRFSS data from CDC

Same process, different data.
```{r}
brfss_2010 = 
  GET("https://chronicdata.cdc.gov/resource/acme-vg9e.csv",
      query =list("$limit" = 5000)) %>% 
  content("parsed")
```

Check the imported data compared to what you expected; is something off? If so, maybe check the API parameters (is there a limit to how many rows you can import by default?).

## Some data aren't so nice

Let's look at some Pokemon!

```{r}
pokemon = 
  GET("https://pokeapi.co/api/v2/pokemon/1") %>% 
  content()

pokemon$name
pokemon$height
pokemon$abilities
```

## Closing thoughts

It can be helpful to split data requests in one markdown, and data manip/analysis in another. Every time you knit, you pull data anew, which can be a lot. Plus, requesting too much data too quickly can cause issues (or cause you to be blocked). Be reasonable.
